{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e52fbaebdf754d47ba01573d3d1283b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cfe25b962d1b44dd9b732b74970730df",
              "IPY_MODEL_5d543e24b41547878f0c55ab308ae44e",
              "IPY_MODEL_0602a177b403414ebc18bf87ec8a65c3"
            ],
            "layout": "IPY_MODEL_aa6ae5d4160241e186056abe283e6264"
          }
        },
        "cfe25b962d1b44dd9b732b74970730df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b383c99f37f49e59a9c65d890421bbb",
            "placeholder": "​",
            "style": "IPY_MODEL_057ed1ea08fd4cebacf31299b244d58f",
            "value": "100%"
          }
        },
        "5d543e24b41547878f0c55ab308ae44e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1ff82a5f01b9453fa8d46de2fa9dcbc7",
            "max": 182040794,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_92f617e2ea8948b4a0f800b895543609",
            "value": 182040794
          }
        },
        "0602a177b403414ebc18bf87ec8a65c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c0f3ad327a0465291aaf96400f75745",
            "placeholder": "​",
            "style": "IPY_MODEL_b65343798e524b4587cbc0d1e08fa104",
            "value": " 182040794/182040794 [00:13&lt;00:00, 19559469.08it/s]"
          }
        },
        "aa6ae5d4160241e186056abe283e6264": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b383c99f37f49e59a9c65d890421bbb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "057ed1ea08fd4cebacf31299b244d58f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1ff82a5f01b9453fa8d46de2fa9dcbc7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92f617e2ea8948b4a0f800b895543609": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0c0f3ad327a0465291aaf96400f75745": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b65343798e524b4587cbc0d1e08fa104": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "j7GQg2SgPXi0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4226e906-28b7-4595-f736-cb5d5f77e5c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import copy\n",
        "import json\n",
        "import math\n",
        "import warnings\n",
        "import numpy as np\n",
        "from scipy import linalg\n",
        "from tqdm import tqdm, trange\n",
        "from scipy.stats import entropy\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "import torch.nn.functional as F\n",
        "from torchvision import transforms\n",
        "from torch.autograd import Variable\n",
        "from torchvision.datasets import SVHN\n",
        "from torchvision.models import inception_v3\n",
        "from torchvision.utils import save_image, make_grid"
      ],
      "metadata": {
        "id": "d_-oNSzkPihT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "outputId": "274b1af9-d379-48d7-90b2-957c10ba6202"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 1.8 MB 15.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 181 kB 88.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 90.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 157 kB 100.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 156 kB 106.6 MB/s \n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport wandb\\n#!wandb login --relogin\\nwandb.login()\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In order to log the values of discriminator and generator loss using the weights and biases, a user account has to be created under weights and biases api."
      ],
      "metadata": {
        "id": "_7q8uZeElOKP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -qU\n",
        "\n",
        "import wandb\n",
        "#!wandb login --relogin\n",
        "wandb.login()\n"
      ],
      "metadata": {
        "id": "wzhCBkkmlC8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Initialising weights and biases to track generation and discrimination loss\n",
        "wandb.init(project=\"Diff_SVHN\",name='Loss_curve',config={\n",
        "      \"learning_rate\": 0.0001,\n",
        "      \"architecture\": \"Diffusion\",\n",
        "      \"dataset\": \"SVHN\",\n",
        "      \"epochs\": 100010,\n",
        "      }) "
      ],
      "metadata": {
        "id": "7JZrcwZ9GlNR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "44c25592-237f-4b68-c4f1-3a56b530bbdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nwandb.init(project=\"Diff_SVHN\",name=\\'Loss_curve\\',config={\\n      \"learning_rate\": 0.0001,\\n      \"architecture\": \"Diffusion\",\\n      \"dataset\": \"SVHN\",\\n      \"epochs\": 100010,\\n      }) \\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms.transforms import Resize\n",
        "from torch.utils.data import DataLoader\n",
        "def svhndata(batch_size, num_workers):\n",
        "  transform=transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "        ])\n",
        "  dataset = SVHN('data/svhn', transform=transform, split='train', download=True)\n",
        "  dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n",
        "  return dataloader\n",
        "\n",
        "def CIFAR10(batch_size, num_workers):\n",
        "  transform=transforms.Compose([\n",
        "            transforms.ToTensor(),\n",
        "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "        ])\n",
        "  dataset = CIFAR10('data/svhn', transform=transform, train=True, download=True)\n",
        "  dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=num_workers)\n",
        "  return dataloader\n",
        "\n"
      ],
      "metadata": {
        "id": "seCwt3PNSZKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionBlock(nn.Module):\n",
        "    def __init__(self, in_ch):\n",
        "        super().__init__()\n",
        "        self.group_norm = nn.GroupNorm(32, in_ch)\n",
        "        self.proj_q = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)\n",
        "        self.proj_k = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)\n",
        "        self.proj_v = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)\n",
        "        self.proj = nn.Conv2d(in_ch, in_ch, 1, stride=1, padding=0)\n",
        "        self.initialize()\n",
        "\n",
        "    def initialize(self):\n",
        "        for module in [self.proj_q, self.proj_k, self.proj_v, self.proj]:\n",
        "            init.xavier_uniform_(module.weight)\n",
        "            init.zeros_(module.bias)\n",
        "        init.xavier_uniform_(self.proj.weight, gain=1e-5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        h = self.group_norm(x)\n",
        "        q = self.proj_q(h)\n",
        "        k = self.proj_k(h)\n",
        "        v = self.proj_v(h)\n",
        "\n",
        "        q = q.permute(0, 2, 3, 1).view(B, H * W, C)\n",
        "        k = k.view(B, C, H * W)\n",
        "        w = torch.bmm(q, k) * (int(C) ** (-0.5))\n",
        "        assert list(w.shape) == [B, H * W, H * W]\n",
        "        w = F.softmax(w, dim=-1)\n",
        "\n",
        "        v = v.permute(0, 2, 3, 1).view(B, H * W, C)\n",
        "        h = torch.bmm(w, v)\n",
        "        assert list(h.shape) == [B, H * W, C]\n",
        "        h = h.view(B, H, W, C).permute(0, 3, 1, 2)\n",
        "        h = self.proj(h)\n",
        "\n",
        "        return x + h\n",
        "\n",
        "class UpSample(nn.Module):\n",
        "    def __init__(self, in_ch):\n",
        "        super().__init__()\n",
        "        self.main = nn.Conv2d(in_ch, in_ch, 3, stride=1, padding=1)\n",
        "        self.initialize()\n",
        "\n",
        "    def initialize(self):\n",
        "        init.xavier_uniform_(self.main.weight)\n",
        "        init.zeros_(self.main.bias)\n",
        "\n",
        "    def forward(self, x, temb):\n",
        "        _, _, H, W = x.shape\n",
        "        x = F.interpolate(\n",
        "            x, scale_factor=2, mode='nearest')\n",
        "        x = self.main(x)\n",
        "        return x\n",
        "\n",
        "class DownSample(nn.Module):\n",
        "    def __init__(self, in_ch):\n",
        "        super().__init__()\n",
        "        self.main = nn.Conv2d(in_ch, in_ch, 3, stride=2, padding=1)\n",
        "        self.initialize()\n",
        "\n",
        "    def initialize(self):\n",
        "        init.xavier_uniform_(self.main.weight)\n",
        "        init.zeros_(self.main.bias)\n",
        "\n",
        "    def forward(self, x, temb):\n",
        "        x = self.main(x)\n",
        "        return x\n",
        "\n",
        "class Embedding(nn.Module):\n",
        "    def __init__(self, T, d_model, dim):\n",
        "        assert d_model % 2 == 0\n",
        "        super().__init__()\n",
        "        emb = torch.arange(0, d_model, step=2) / d_model * math.log(10000)\n",
        "        emb = torch.exp(-emb)\n",
        "        pos = torch.arange(T).float()\n",
        "        emb = pos[:, None] * emb[None, :]\n",
        "        assert list(emb.shape) == [T, d_model // 2]\n",
        "        emb = torch.stack([torch.sin(emb), torch.cos(emb)], dim=-1)\n",
        "        assert list(emb.shape) == [T, d_model // 2, 2]\n",
        "        emb = emb.view(T, d_model)\n",
        "\n",
        "        self.timembedding = nn.Sequential(\n",
        "            nn.Embedding.from_pretrained(emb),\n",
        "            nn.Linear(d_model, dim),\n",
        "            Shift(),\n",
        "            nn.Linear(dim, dim),\n",
        "        )\n",
        "        self.initialize()\n",
        "\n",
        "    def initialize(self):\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, nn.Linear):\n",
        "                init.xavier_uniform_(module.weight)\n",
        "                init.zeros_(module.bias)\n",
        "\n",
        "    def forward(self, t):\n",
        "        emb = self.timembedding(t)\n",
        "        return emb\n",
        "\n",
        "\n",
        "class Shift(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x * torch.sigmoid(x)\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_ch, out_ch, tdim, dropout, attn=False):\n",
        "        super().__init__()\n",
        "        self.block1 = nn.Sequential(\n",
        "            nn.GroupNorm(32, in_ch),\n",
        "            Shift(),\n",
        "            nn.Conv2d(in_ch, out_ch, 3, stride=1, padding=1),\n",
        "        )\n",
        "        self.temb_proj = nn.Sequential(\n",
        "            Shift(),\n",
        "            nn.Linear(tdim, out_ch),\n",
        "        )\n",
        "        self.block2 = nn.Sequential(\n",
        "            nn.GroupNorm(32, out_ch),\n",
        "            Shift(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Conv2d(out_ch, out_ch, 3, stride=1, padding=1),\n",
        "        )\n",
        "        if in_ch != out_ch:\n",
        "            self.shortcut = nn.Conv2d(in_ch, out_ch, 1, stride=1, padding=0)\n",
        "        else:\n",
        "            self.shortcut = nn.Identity()\n",
        "        if attn:\n",
        "            self.attn = AttentionBlock(out_ch)\n",
        "        else:\n",
        "            self.attn = nn.Identity()\n",
        "        self.initialize()\n",
        "\n",
        "    def initialize(self):\n",
        "        for module in self.modules():\n",
        "            if isinstance(module, (nn.Conv2d, nn.Linear)):\n",
        "                init.xavier_uniform_(module.weight)\n",
        "                init.zeros_(module.bias)\n",
        "        init.xavier_uniform_(self.block2[-1].weight, gain=1e-5)\n",
        "\n",
        "    def forward(self, x, temb):\n",
        "        h = self.block1(x)\n",
        "        h += self.temb_proj(temb)[:, :, None, None]\n",
        "        h = self.block2(h)\n",
        "\n",
        "        h = h + self.shortcut(x)\n",
        "        h = self.attn(h)\n",
        "        return h\n",
        "\n",
        "\n",
        "class UNet(nn.Module):\n",
        "    def __init__(self, T, ch, ch_mult, attn, num_res_blocks, dropout):\n",
        "        super().__init__()\n",
        "        assert all([i < len(ch_mult) for i in attn]), 'attn index out of bound'\n",
        "        tdim = ch * 4\n",
        "        self.time_embedding = Embedding(T, ch, tdim)\n",
        "\n",
        "        self.head = nn.Conv2d(3, ch, kernel_size=3, stride=1, padding=1)\n",
        "        self.downblocks = nn.ModuleList()\n",
        "        chs = [ch]\n",
        "        now_ch = ch\n",
        "        for i, mult in enumerate(ch_mult):\n",
        "            out_ch = ch * mult\n",
        "            for _ in range(num_res_blocks):\n",
        "                self.downblocks.append(ResidualBlock(\n",
        "                    in_ch=now_ch, out_ch=out_ch, tdim=tdim,\n",
        "                    dropout=dropout, attn=(i in attn)))\n",
        "                now_ch = out_ch\n",
        "                chs.append(now_ch)\n",
        "            if i != len(ch_mult) - 1:\n",
        "                self.downblocks.append(DownSample(now_ch))\n",
        "                chs.append(now_ch)\n",
        "\n",
        "        self.middleblocks = nn.ModuleList([\n",
        "            ResidualBlock(now_ch, now_ch, tdim, dropout, attn=True),\n",
        "            ResidualBlock(now_ch, now_ch, tdim, dropout, attn=False),\n",
        "        ])\n",
        "\n",
        "        self.upblocks = nn.ModuleList()\n",
        "        for i, mult in reversed(list(enumerate(ch_mult))):\n",
        "            out_ch = ch * mult\n",
        "            for _ in range(num_res_blocks + 1):\n",
        "                self.upblocks.append(ResidualBlock(\n",
        "                    in_ch=chs.pop() + now_ch, out_ch=out_ch, tdim=tdim,\n",
        "                    dropout=dropout, attn=(i in attn)))\n",
        "                now_ch = out_ch\n",
        "            if i != 0:\n",
        "                self.upblocks.append(UpSample(now_ch))\n",
        "        assert len(chs) == 0\n",
        "\n",
        "        self.tail = nn.Sequential(\n",
        "            nn.GroupNorm(32, now_ch),\n",
        "            Shift(),\n",
        "            nn.Conv2d(now_ch, 3, 3, stride=1, padding=1)\n",
        "        )\n",
        "        self.initialize()\n",
        "\n",
        "    def initialize(self):\n",
        "        init.xavier_uniform_(self.head.weight)\n",
        "        init.zeros_(self.head.bias)\n",
        "        init.xavier_uniform_(self.tail[-1].weight, gain=1e-5)\n",
        "        init.zeros_(self.tail[-1].bias)\n",
        "\n",
        "    def forward(self, x, t):\n",
        "        temb = self.time_embedding(t)\n",
        "        h = self.head(x)\n",
        "        hs = [h]\n",
        "        for layer in self.downblocks:\n",
        "            h = layer(h, temb)\n",
        "            hs.append(h)\n",
        "        for layer in self.middleblocks:\n",
        "            h = layer(h, temb)\n",
        "        for layer in self.upblocks:\n",
        "            if isinstance(layer, ResidualBlock):\n",
        "                h = torch.cat([h, hs.pop()], dim=1)\n",
        "            h = layer(h, temb)\n",
        "        h = self.tail(h)\n",
        "\n",
        "        assert len(hs) == 0\n",
        "        return h"
      ],
      "metadata": {
        "id": "BlQdW2WFfllR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QUeKzbX1fhdq"
      },
      "outputs": [],
      "source": [
        "class GaussianDiffusionTrainer(nn.Module):\n",
        "    def __init__(self, model, beta_1, beta_T, T):\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = model\n",
        "        self.T = T\n",
        "\n",
        "        self.register_buffer(\n",
        "            'betas', torch.linspace(beta_1, beta_T, T).double())\n",
        "        alphas = 1. - self.betas\n",
        "        alphas_bar = torch.cumprod(alphas, dim=0)\n",
        "        self.register_buffer(\n",
        "            'sqrt_alphas_bar', torch.sqrt(alphas_bar))\n",
        "        self.register_buffer(\n",
        "            'sqrt_one_minus_alphas_bar', torch.sqrt(1. - alphas_bar))\n",
        "\n",
        "    def forward(self, x_0):\n",
        "        t = torch.randint(self.T, size=(x_0.shape[0], ), device=x_0.device)\n",
        "        noise = torch.randn_like(x_0)\n",
        "        x_t = (\n",
        "            extract(self.sqrt_alphas_bar, t, x_0.shape) * x_0 +\n",
        "            extract(self.sqrt_one_minus_alphas_bar, t, x_0.shape) * noise)\n",
        "        loss = F.mse_loss(self.model(x_t, t), noise, reduction='none')\n",
        "        return loss\n",
        "\n",
        "\n",
        "class GaussianDiffusionSampler(nn.Module):\n",
        "    def __init__(self, model, beta_1, beta_T, T, img_size=32,\n",
        "                 mean_type='eps', var_type='fixedlarge'):\n",
        "        assert mean_type in ['epsilon']\n",
        "        assert var_type in ['fixedlarge']\n",
        "        super().__init__()\n",
        "\n",
        "        self.model = model\n",
        "        self.T = T\n",
        "        self.img_size = img_size\n",
        "        self.mean_type = mean_type\n",
        "        self.var_type = var_type\n",
        "\n",
        "        self.register_buffer(\n",
        "            'betas', torch.linspace(beta_1, beta_T, T).double())\n",
        "        alphas = 1. - self.betas\n",
        "        alphas_bar = torch.cumprod(alphas, dim=0)\n",
        "        alphas_bar_prev = F.pad(alphas_bar, [1, 0], value=1)[:T]\n",
        "        self.register_buffer(\n",
        "            'sqrt_recip_alphas_bar', torch.sqrt(1. / alphas_bar))\n",
        "        self.register_buffer(\n",
        "            'sqrt_recipm1_alphas_bar', torch.sqrt(1. / alphas_bar - 1))\n",
        "        self.register_buffer(\n",
        "            'posterior_var',\n",
        "            self.betas * (1. - alphas_bar_prev) / (1. - alphas_bar))\n",
        "        self.register_buffer(\n",
        "            'posterior_log_var_clipped',\n",
        "            torch.log(\n",
        "                torch.cat([self.posterior_var[1:2], self.posterior_var[1:]])))\n",
        "        self.register_buffer(\n",
        "            'posterior_mean_coef1',\n",
        "            torch.sqrt(alphas_bar_prev) * self.betas / (1. - alphas_bar))\n",
        "        self.register_buffer(\n",
        "            'posterior_mean_coef2',\n",
        "            torch.sqrt(alphas) * (1. - alphas_bar_prev) / (1. - alphas_bar))\n",
        "\n",
        "    def q_mean_variance(self, x_0, x_t, t):\n",
        "        assert x_0.shape == x_t.shape\n",
        "        posterior_mean = (\n",
        "            extract(self.posterior_mean_coef1, t, x_t.shape) * x_0 +\n",
        "            extract(self.posterior_mean_coef2, t, x_t.shape) * x_t\n",
        "        )\n",
        "        posterior_log_var_clipped = extract(\n",
        "            self.posterior_log_var_clipped, t, x_t.shape)\n",
        "        return posterior_mean, posterior_log_var_clipped\n",
        "\n",
        "    def predict_xstart_from_eps(self, x_t, t, eps):\n",
        "        assert x_t.shape == eps.shape\n",
        "        return (\n",
        "            extract(self.sqrt_recip_alphas_bar, t, x_t.shape) * x_t -\n",
        "            extract(self.sqrt_recipm1_alphas_bar, t, x_t.shape) * eps\n",
        "        )\n",
        "\n",
        "    def predict_xstart_from_xprev(self, x_t, t, xprev):\n",
        "        assert x_t.shape == xprev.shape\n",
        "        return (\n",
        "            extract(\n",
        "                1. / self.posterior_mean_coef1, t, x_t.shape) * xprev -\n",
        "            extract(\n",
        "                self.posterior_mean_coef2 / self.posterior_mean_coef1, t,\n",
        "                x_t.shape) * x_t\n",
        "        )\n",
        "\n",
        "    def p_mean_variance(self, x_t, t):\n",
        "        model_log_var = {\n",
        "            'fixedlarge': torch.log(torch.cat([self.posterior_var[1:2],\n",
        "                                               self.betas[1:]]))\n",
        "        }[self.var_type]\n",
        "        model_log_var = extract(model_log_var, t, x_t.shape)\n",
        "        if self.mean_type == 'xprev':\n",
        "            x_prev = self.model(x_t, t)\n",
        "            x_0 = self.predict_xstart_from_xprev(x_t, t, xprev=x_prev)\n",
        "            model_mean = x_prev\n",
        "        elif self.mean_type == 'xstart':\n",
        "            x_0 = self.model(x_t, t)\n",
        "            model_mean, _ = self.q_mean_variance(x_0, x_t, t)\n",
        "        elif self.mean_type == 'epsilon':\n",
        "            eps = self.model(x_t, t)\n",
        "            x_0 = self.predict_xstart_from_eps(x_t, t, eps=eps)\n",
        "            model_mean, _ = self.q_mean_variance(x_0, x_t, t)\n",
        "        else:\n",
        "            raise NotImplementedError(self.mean_type)\n",
        "        x_0 = torch.clip(x_0, -1., 1.)\n",
        "\n",
        "        return model_mean, model_log_var\n",
        "\n",
        "    def forward(self, x_T):\n",
        "        x_t = x_T\n",
        "        for time_step in reversed(range(self.T)):\n",
        "            t = x_t.new_ones([x_T.shape[0], ], dtype=torch.long) * time_step\n",
        "            mean, log_var = self.p_mean_variance(x_t=x_t, t=t)\n",
        "            if time_step > 0:\n",
        "                noise = torch.randn_like(x_t)\n",
        "            else:\n",
        "                noise = 0\n",
        "            x_t = mean + torch.exp(0.5 * log_var) * noise\n",
        "        x_0 = x_t\n",
        "        return torch.clip(x_0, -1, 1)\n",
        "\n",
        "def extract(v, t, x_shape):\n",
        "    out = torch.gather(v, index=t, dim=0).float()\n",
        "    return out.view([t.shape[0]] + [1] * (len(x_shape) - 1))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FID():\n",
        "    \n",
        "    def __init__(self, cache_dir='./Cache',  device='cpu',transform_input=True):\n",
        "        os.environ[\"TORCH_HOME\"] = \"./Cache\"\n",
        "        self.device=device\n",
        "        self.transform_input = transform_input\n",
        "        self.InceptionV3 = inception_v3(pretrained=True, transform_input=False, aux_logits=True).to(device=self.device)\n",
        "        self.InceptionV3.eval()\n",
        "    \n",
        "    def inception_network(self, x):\n",
        "        # Resize to Fit InceptionV3\n",
        "        if list(x.shape[-2:]) != [299,299]:\n",
        "            with warnings.catch_warnings():\n",
        "                warnings.simplefilter(\"ignore\")\n",
        "                x = F.interpolate(x, size=[299,299], mode='bilinear')\n",
        "        # Transform Input to InceptionV3 Standards\n",
        "        if self.transform_input:\n",
        "            a = torch.unsqueeze(x[:, 0], 1) * (0.229 / 0.5) + (0.485 - 0.5) / 0.5\n",
        "            b = torch.unsqueeze(x[:, 1], 1) * (0.224 / 0.5) + (0.456 - 0.5) / 0.5\n",
        "            c = torch.unsqueeze(x[:, 2], 1) * (0.225 / 0.5) + (0.406 - 0.5) / 0.5\n",
        "            x = torch.cat((a,b,c), 1)\n",
        "        # Run Through Partial InceptionV3 Model\n",
        "        with torch.no_grad():\n",
        "            # N x 3 x 299 x 299\n",
        "            x = self.InceptionV3.Conv2d_1a_3x3(x)\n",
        "            # N x 32 x 149 x 149\n",
        "            x = self.InceptionV3.Conv2d_2a_3x3(x)\n",
        "            # N x 32 x 147 x 147\n",
        "            x = self.InceptionV3.Conv2d_2b_3x3(x)\n",
        "            # N x 64 x 147 x 147\n",
        "            x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
        "            # N x 64 x 73 x 73\n",
        "            x = self.InceptionV3.Conv2d_3b_1x1(x)\n",
        "            # N x 80 x 73 x 73\n",
        "            x = self.InceptionV3.Conv2d_4a_3x3(x)\n",
        "            # N x 192 x 71 x 71\n",
        "            x = F.max_pool2d(x, kernel_size=3, stride=2)\n",
        "            # N x 192 x 35 x 35\n",
        "            x = self.InceptionV3.Mixed_5b(x)\n",
        "            # N x 256 x 35 x 35\n",
        "            x = self.InceptionV3.Mixed_5c(x)\n",
        "            # N x 288 x 35 x 35\n",
        "            x = self.InceptionV3.Mixed_5d(x)\n",
        "            # N x 288 x 35 x 35\n",
        "            x = self.InceptionV3.Mixed_6a(x)\n",
        "            # N x 768 x 17 x 17\n",
        "            x = self.InceptionV3.Mixed_6b(x)\n",
        "            # N x 768 x 17 x 17\n",
        "            x = self.InceptionV3.Mixed_6c(x)\n",
        "            # N x 768 x 17 x 17\n",
        "            x = self.InceptionV3.Mixed_6d(x)\n",
        "            # N x 768 x 17 x 17\n",
        "            x = self.InceptionV3.Mixed_6e(x)\n",
        "            # N x 768 x 17 x 17\n",
        "            x = self.InceptionV3.Mixed_7a(x)\n",
        "            # N x 1280 x 8 x 8\n",
        "            x = self.InceptionV3.Mixed_7b(x)\n",
        "            # N x 2048 x 8 x 8\n",
        "            x = self.InceptionV3.Mixed_7c(x)\n",
        "            # N x 2048 x 8 x 8\n",
        "            # Adaptive average pooling\n",
        "            x = F.adaptive_avg_pool2d(x, (1, 1))\n",
        "            # N x 2048 x 1 x 1\n",
        "            return x\n",
        "  \n",
        "\n",
        "    def compute_fid(self, real_img, generated_img, batch_size=64):\n",
        "        # Ensure Set Sizes are the Same\n",
        "        assert(real_img.shape[0] == generated_img.shape[0])\n",
        "        # Build Random Sampling Orders\n",
        "        real_img = real_img[np.random.permutation(real_img.shape[0])]\n",
        "        generated_imag = generated_img[np.random.permutation(generated_img.shape[0])]\n",
        "        # Lists of Maps per Batch\n",
        "        real_features = []\n",
        "        gen_features = []\n",
        "        # Build Maps\n",
        "        for s in range(int(math.ceil(real_img.shape[0]/batch_size))):\n",
        "            indx = np.arange(batch_size*s, min(batch_size*(s+1), real_img.shape[0]))\n",
        "            real_features.append(self.inception_network(real_img[indx].to(device=self.device)).detach().to(device='cpu'))\n",
        "            gen_features.append(self.inception_network(generated_imag[indx].to(device=self.device)).detach().to(device='cpu'))\n",
        "\n",
        "        # Concatenate Maps\n",
        "        real_features = np.squeeze(torch.cat(real_features).numpy())\n",
        "        gen_features = np.squeeze(torch.cat(gen_features).numpy())\n",
        "        # Calculate FID\n",
        "        # Activation Statistics\n",
        "        mu_g = np.mean(gen_features, axis=0)\n",
        "        mu_x = np.mean(real_features, axis=0)\n",
        "        sigma_g = np.cov(gen_features, rowvar=False)\n",
        "        sigma_x = np.cov(real_features, rowvar=False)\n",
        "        # Sum of Squared Differences\n",
        "        ssd = np.sum((mu_g - mu_x)**2)\n",
        "        # Square Root of Product of Covariances\n",
        "        covmean = linalg.sqrtm(sigma_g.dot(sigma_x), disp=False)[0]\n",
        "        if np.iscomplexobj(covmean):\n",
        "            covmean = covmean.real\n",
        "        # Final FID Computation\n",
        "        return ssd + np.trace(sigma_g + sigma_x - 2*covmean)\n",
        "fid_ = FID()"
      ],
      "metadata": {
        "id": "R4gCdRjRne5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def show_tensor_images(image_tensor, num_images=8, size=(1, 32, 32)):\n",
        "    image_tensor = (image_tensor + 1) / 2\n",
        "\n",
        "    image_unflat = image_tensor.detach().cpu()\n",
        "\n",
        "    image_grid = make_grid(image_unflat[:num_images], nrow=4)\n",
        "    img = image_grid.permute(1, 2, 0).squeeze()\n",
        "    \n",
        "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "LLKXov8TkNhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def warmup_lr(step):\n",
        "    return min(step, warmup) / warmup"
      ],
      "metadata": {
        "id": "51PvhxoDjNkk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "var_type = 'fixedlarge'\n",
        "batch_size = 64\n",
        "num_workers = 3\n",
        "channels = 64\n",
        "grad_clip = 1.\n",
        "img_size = 32\n",
        "warmup = 5000\n",
        "learning_rate = 0.001\n",
        "total_steps = 100000\n",
        "batch_size = 64\n",
        "num_workers = 2\n",
        "sample_size = 64\n",
        "dropout = 0.1\n",
        "mean_type = 'epsilon'\n",
        "channels_mult = [1, 2, 2, 2]\n",
        "attention = [1]\n",
        "num_res_blocks = 2\n",
        "beta_1 = 1e-4\n",
        "beta_T = 0.02\n",
        "T = 3000\n",
        "device = 'cuda'\n",
        "\n",
        "\n",
        "\n",
        "unet = UNet(\n",
        "        T=T, ch=channels, ch_mult=channels_mult, attn=attention,\n",
        "        num_res_blocks=num_res_blocks, dropout=dropout)\n",
        "optimizer = torch.optim.Adam(unet.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=warmup_lr)\n",
        "trainer = GaussianDiffusionTrainer(\n",
        "        unet, beta_1, beta_T, T).to(device)\n",
        "net_sampler = GaussianDiffusionSampler(\n",
        "        unet, beta_1, beta_T, T, img_size,\n",
        "        mean_type, var_type).to(device)\n",
        "\n",
        "with trange(total_steps, dynamic_ncols=True) as pbar:\n",
        "    for step in pbar:\n",
        "      optimizer.zero_grad()\n",
        "      x_0 = next(iter(dataloader)).to(device)\n",
        "      loss = trainer(x_0).mean()\n",
        "      loss.backward()\n",
        "      torch.nn.utils.clip_grad_norm_(\n",
        "          unet.parameters(), grad_clip)\n",
        "      optimizer.step()\n",
        "      scheduler.step()\n",
        "\n",
        "      # log loss \n",
        "      wandb.log({\"Diffusion Loss\": loss.item()})\n",
        "      pbar.set_postfix(loss='%.3f' % loss.item())\n",
        "      x_T = torch.randn(sample_size, 3, img_size, img_size)\n",
        "      x_T = x_T.to(device)\n",
        "\n",
        "      if step % 5000 == 0 and step >0:\n",
        "         unet.eval()\n",
        "         with torch.no_grad():\n",
        "             gen = net_sampler(x_T)\n",
        "             show_tensor_images(gen)\n",
        "                    \n",
        "         unet.train()\n",
        "         save_image(gen, '/content/drive/MyDrive/DiffusionModel_SVHN_Gen/Iter{}.png'.format(step))\n",
        "         fid_.compute_fid(x_0, gen)"
      ],
      "metadata": {
        "id": "8qx5I2zDfr_M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 66,
          "referenced_widgets": [
            "e52fbaebdf754d47ba01573d3d1283b2",
            "cfe25b962d1b44dd9b732b74970730df",
            "5d543e24b41547878f0c55ab308ae44e",
            "0602a177b403414ebc18bf87ec8a65c3",
            "aa6ae5d4160241e186056abe283e6264",
            "4b383c99f37f49e59a9c65d890421bbb",
            "057ed1ea08fd4cebacf31299b244d58f",
            "1ff82a5f01b9453fa8d46de2fa9dcbc7",
            "92f617e2ea8948b4a0f800b895543609",
            "0c0f3ad327a0465291aaf96400f75745",
            "b65343798e524b4587cbc0d1e08fa104"
          ]
        },
        "outputId": "f0940ad2-33b8-4f06-93b5-740263c85d75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://ufldl.stanford.edu/housenumbers/train_32x32.mat to data/svhn/train_32x32.mat\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/182040794 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e52fbaebdf754d47ba01573d3d1283b2"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yCvJlMT0kInB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}